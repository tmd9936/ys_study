remotes::install_github('haven-jeon/KoNLP', upgrade="never", INSTALL_opts = c("--no-multiarch"))
install.packages("remotes")
remotes::install_github('haven-jeon/KoNLP', upgrade="never", INSTALL_opts = c("--no-multiarch"))
library(rJava)
library(stringr)
library(KoNLP)
useSejongDic()
library(rlang)
install.packages("rlang")
library(rJava)
library(stringr)
library(KoNLP)
library(rlang)
useSejongDic()
library(rJava)
library(stringr)
library(KoNLP)
library(KoNLP)
install.packages("rlang")
library(KoNLP)
extractNoun("롯데마트 판매합니다다")
extractNoun("롯데마트 판매하하")
extractNoun("롯데마트 판매하")
txt <- readLines("hiphop.txt")
txt <- readLines("hiphop.txt")
head(txt)
txt <- str_replace_all(txt,"\\W", " ")
txt
nouns <- extractNoun(txt)
nouns
ulist(nouns)
unlist(nouns)
table(unlist(nouns))
wordCount <- table(unlist(nouns))
wordCount
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word
library(dplyr)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word
df_word <- filter(df_word, nchar(word) >= 2)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq))
df_word <- filter(df_word, nchar(word) >= 2)
top_20
df_word <- filter(df_word, nchar(word) >= 2)
library(dplyr)
library(stringr)
df_word <- filter(df_word, nchar(word) >= 2)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
library(wordcloud)
set.seed(100)
wordCount <- table(unlist(nouns))
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
df_word
set.seed(100)
pal <- brewer.pal(9, "Set1")
word <- c("인천광역시", "서울시", "대구시")
freq <- c(700, 80, 30)
wordcloud(word, freq, colors = pal, rot.per = .1)
df_word <- as.data.frame(wordCount, stringsAsFactors = F)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
top_20
word
wordcloud(top_20, freq, colors = pal, rot.per = .1)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(rJava)
library(stringr)
library(KoNLP)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
txt <- readLines("hiphop.txt")
txt <- str_replace_all(txt,"\\W", " ")
nouns <- extractNoun(txt)
nouns
wordCount <- table(unlist(nouns))
wordCount
df_word <- as.data.frame(wordCount, stringsAsFactors = F)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
set.seed(100)
pal <- brewer.pal(9, "Set1")
wordcloud(word, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
wordcloud(top_20$word, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
useSejongDic()
text <- LeadLines(file.choose())
text <- LeadLines(file.choose())
library(readxl)
movies <- read_excel("movies.xlsx")
movies <- read_excel("movies.xlsx")
movies
head(movies)
tail(movies)
names(movies)
dim(movies)
summary(movies)
max(movies$매출액)
max(movies$매출액)
max(movies$관객수)
max(movies$관객수)
max(movies$상상영횟수)
max(movies$상상영횟수)
max(movies$상상영횟수)
max(movies$상영횟수)
which.max(movies$관객수)
which.max(movies$관객수)
movies[64, 1]
movies[64]
movies[64]
movies[64, ]
which.min(movies$스크린수)
which.min(movies$스크린수)
movies[60, ]
movies$관객수수
movies$관객수수
movies$관객수
movies$관객수
hist(movies$관객수)
library(ggplot)
library(ggplot2)
names(movies)
ggplot(data = movies, aes(x = 관객수, y=매출액)) + geom_col()
ggplot(data = movies, aes(x = 관객수, y=매출액)) + geom_histogram()
ggplot(data = movies, aes(x = 관객수)) + geom_histogram()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_dotplot()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_bar()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_area()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_line()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_line()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_point()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수, col=대표국적)) + geom_point()
install.packages("plotly")
library(plotly)
## 관객수 상영 횟수
gp <- ggplot(data = movies, aes(x = 관객수, y=상영횟수, col=대표국적)) + geom_point()
ggplotly(go)
ggplotly(gp)
which.max(mocies$상영횟수수)
which.max(mocies$상영횟수)
which.max(movies$상영횟수)
movies[126,]
which.max(movies$곤객수/movies$상영횟수)
which.max(movies$관관객수/movies$상영횟수)
which.max(movies$관객수/movies$상영횟수)
movies[21,]
# 상영횟수당 관객수가 가장 많은 영화 찾기
movies2 %>%
mutate(상영대비관객 = 관객수/상영횟수)
# 상영횟수당 관객수가 가장 많은 영화 찾기
movies2 <- movies %>%
mutate(상영대비관객 = 관객수/상영횟수)
movies2 %>%
arrange(상영대비관객) %>%
head(10)
movies2 %>%
arrange(상영대비관객,desc) %>%
head(10)
?arrange
movies2 %>%
arrange(desc(상영대비관객)) %>%
head(10)
sort(movies$관객수 / movies$상상영횟수, decreasing = T)
sort(movies$관객수 / movies$상영횟수, decreasing = T)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관개)) %>%
head(10)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관객객)) %>%
head(10)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관객)) %>%
head(10)
name(movies)
names(movies)
### 대응 표본 T 검정
raw_d <- read.csv(file = "data/htest02d.csv", header = T)
raw_d
head(raw_d)
dim(raw_d)
setwd("C:/study/R_lang")
groupA_d <- raw_d[,1]
grouBA_d <- raw_d[,2]
mean(groupA_d)
mean(groupB_d)
groupB_d <- raw_d[,2]
mean(groupB_d)
mean(groupA_d)
mean(groupB_d)
#### 데이터 정규성 검정
shapiro.test(groupA_d)
#### 데이터 정규성 검정
d = groupA_d - groupB_d
d
shapiro.test(d)
# -> p-value가 0.1621 > 0.05이기때문에 정규분포를 따름
qqnorm(d)
qqline(d)
#### 대응표본 t검정
t.test(d)
#### 대응표본 t검정
t.test(groupA_d, groupB_d, alternative = "less")
#### 대응표본 t검정
t.test(groupA_d, groupB_d, alternative = "less", paired = T)
rawN30 = read.csv("data/htest03.csv", header = T)
head(rawN30)
tail(rawN30)
dim(rawN30)
names(rawN30)
groupA3 <- rawN30[rawN30$group == 'A', 1:2]
groupB3 <- rawN30[rawN30$group == 'B', 1:2]
groupA3
groupB3
mean(groupA3$height)
mean(groupB3$height)
cat("mean of x1:", mean_x1, "\n")
z.test <- funtion(x1,x2){
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
n_x1 = length(x1)
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
n_x2 = length(x2)
z.test <- function(x1,x2){
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less")
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
z.test(groupA3$height, groupB3$height)
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
# 위의 데이터를 t-test 했을 경우
# 분산
var.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T, conf.level = 0.95)
#####################################################
# ANOVA 분석 : 여러 집단의 평균차이 검정
# ANOVA 분석을 위해서는 lawstat 패키지가 핑요함
install.packages("lawstat")
library(lawstat)
raw_anova <- read.csv(file= "data/htest04.csv", header = T)
head(raw_anova)
tatil(raw_anova)
tail(raw_anova)
groupA4 <- raw_anova[raw_anova$group=='A', 1:2]
groupB4 <- raw_anova[raw_anova$group=='B', 1:2]
groupC4 <- raw_anova[raw_anova$group=='C', 1:2]
mean(groupA4[,2])
mean(groupB4[,2])
mean(groupC4[,2])
# 정규성 검정
shapiro.test(groupA4[,2])
shapiro.test(groupB4[,2])
shapiro.test(groupC4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupB4[,2])
qqline(groupB4[,2])
qqnorm(groupC4[,2])
qqline(groupC4[,2])
anova(raw_anova)
anova(groupA4$height, groupB4$height, groupC4$height)
levene.test(raw_anova$height, raw_anova$group)
# p-value = 0.3298이므로 분산이 동일하다.
bartlett.test(height~group, data=raw_anova)
# p-value = 0.3435이므로 세집단간의 분산이 동일하다.
rawAnova <- aov(height ~ group, data= raw_anova)
summary(rawAnova)
head(raw_chisq)
###########################################
# 분할표를 이용한 연관성 분석
raw_chisq <- read.csv(file="data/htest05.csv", header = T)
head(raw_chisq)
tail(raw_chisq)
raw_Table <- table(raw_chisq)
raw_Table
# 카이제곱 테스트
chisq.test(raw_Table)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = F)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = FALSE)
# 카이제곱 테스트
chisq.test(raw_Table, correct = FALSE)
library(caret)
rawdata <- read.csv(file="heart.csv", header = T)
str(rawdata)
# 타겟(정답, class) 변수의 범주화
rawdata$target <- as.factor(rawdata$target)
str(rawdata)
# 범주형 독립변수는 범주화
newdata <- rawdata
factVar <- c("sex","cp","fps","restecg","exang","ca","thal")
factVar <- c("sex","cp","fbs","restecg","exang","ca","thal")
newdata_[,factVar] = lapply(newdata[, factVar], factor)
newdata[,factVar] = lapply(newdata[, factVar], factor)
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata$trestbps <- scale(rawdata$age)
rawdata$chol <- scale(rawdata$age)
rawdata$thalach <- scale(rawdata$age)
rawdata$slope <- scale(rawdata$age)
# 타겟(정답, class) 변수의 범주화
rawdata$target <- as.factor(rawdata$target)
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata$trestbps <- scale(rawdata$age)
rawdata$chol <- scale(rawdata$age)
rawdata$thalach <- scale(rawdata$age)
rawdata$slope <- scale(rawdata$age)
# 범주형 독립변수는 범주화
newdata <- rawdata
factVar <- c("sex","cp","fbs","restecg","exang","ca","thal")
newdata[,factVar] = lapply(newdata[, factVar], factor)
# 트레이닝 - 테스트 데이터 분할
set.seed(2020)
datatotal <- sort(sample(nrow(newdata)))
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*0.7))
train <- newdata[datatotal, ]
train <- newdata[-datatotal, ]
train <- newdata[datatotal, ]
test <- newdata[-datatotal, ]
# 학습모델 생성
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LogitBoost",
trControl = ctrl,
metric = "Accuracy")
logitFit
# 트레이닝 - 테스트 데이터 분할
set.seed(2020)
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*0.7))
train <- newdata[datatotal, ]
test <- newdata[-datatotal, ]
# 학습모델 생성
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LogitBoost",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
# "LogitBoost", "LMT", "plr", "regLogistic"
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LMT",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "regLogistic",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "regLogistic",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
