qplot(welfare$birth)
library(foreign)
library(dplyr)
library(ggplot2)
qplot(welfare$birth)
table(welfare$birth)
summary(welfare$birth)
# 이상치가 있는 경우에는 결측처리
welfare$birth <- ifelse(welfare$birth == 9999, NA, welfare$birth)
table(is.na(welfare$birth))
welfare$age <- 2014 - welfare$birth + 1
summary(welfare$age)
qplot(welfare$age)
# 나이별 소득 평균 분석하기
# 평균표 만들기
age_income <- welfare %>%
group_by(age) %>%
summarise(mean_income - mean(income))
# 나이별 소득 평균 분석하기
# 평균표 만들기
age_income <- welfare %>%
group_by(age) %>%
summarise(mean_income = mean(income))
age_income
# 그래프 만들기 - 산점도
ggplot(data = age$income, aes(x= age, y=mean_income)) + geom_point()
# 그래프 만들기 - 산점도
ggplot(data = age_income, aes(x= age, y=mean_income)) + geom_point()
# 연령대 변수 생성
welfare <- welfare %>%
mutate(age_gp = ifelse(age < 30, "young", ifelse(age <= 59, "middle", "old")))
table(welfare$mutate)
table(welfare$age_gp)
# 연령대 변수 생성
welfare <- welfare %>%
mutate(age_gp = ifelse(age < 39, "young", ifelse(age <= 65, "middle", "old")))
table(welfare$age_gp)
qplot(welfare$age_gp)
# 연령대별 소득 평균표 생성(young 제외)
welfare_income <- welfare %>%
filter(age_gp != "young") %>%
group(age_gp) %>%
summarise(mean_income = mean(income))
# 연령대별 소득 평균표 생성(young 제외)
welfare_income <- welfare %>%
filter(age_gp != "young") %>%
group_by(age_gp) %>%
summarise(mean_income = mean(income))
welfare_income
ggplot(data = welfare_income, aes(x = age_gp, y = mean_income)) + geom_bar()
ggplot(data = welfare_income, aes(x = age_gp, y = mean_income)) + geom_bar()
ggplot(data = welfare_income, aes(x = age_gp)) + geom_bar()
ggplot(data = welfare_income, aes(x = age_gp, y = mean_income)) + geom_bar()
ggplot(data = welfare_income, aes(x = age_gp, y = mean_income)) + geom_bar()
ggplot(data = welfare_income, aes(x = age_gp, y = mean_income)) + geom_col()
welfare_income <- welfare %>%
filter(age_gp != "young") %>%
group_by(age_gp, sex) %>%
summarise(mean_income = mean(income))
welfare_income
ggplot(data = welfare_income, aes(x=age_gp. y= mean_income, fill=sex)) +geom_col(position = "dodge")
ggplot(data = welfare_income, aes(x=age_gp, y= mean_income, fill=sex)) +geom_col(position = "dodge")
ggplot(data = welfare_income, aes(x=age_gp, y= mean_income, fill=sex)) +geom_col(position = "stack")
ggplot(data = welfare_income, aes(x=age_gp, y= mean_income, fill=sex)) +geom_col(position = "dodge")
sex_age <- welfare %>%
fileter(is.na(income)) %>%
group_by(age,sex) %>%
summarise(mean_income = mean(income))
sex_age <- welfare %>%
filter(is.na(income)) %>%
group_by(age,sex) %>%
summarise(mean_income = mean(income))
sex_age
sex_age <- welfare %>%
filter(!is.na(income)) %>%
group_by(age,sex) %>%
summarise(mean_income = mean(income))
sex_age
ggplot(data = sex_age, aes(x=age, y= mean_income, col = sex)) + geom_line()
ggplot(data = sex_age, aes(x=age, y= mean_income, col = sex)) + geom_line() + xlim(20,90)
ggplot(data=mpg, aes(x=class)) + geom_bar()
?reorder
ggplot(data=mpg, aes(x=class)) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class, count))) + geom_bar()
?reorder
ggplot(data=mpg, aes(x=reorder(class, count))) + geom_bar()
ggplot(data=mpg, aes(x=class)) + geom_bar()
?aes
?reorder
ggplot(data=mpg, aes(x=reorder(class,count(class)))) + geom_bar()
ggplot(data=mpg, aes(x=class)) + geom_bar()
?reorder
ggplot(data=mpg, aes(x=reorder(class,order = is.ordered(x)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class,x,order = is.ordered(x)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class,count(class),order = is.ordered(x)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class,mean(count(class)),order = is.ordered(x)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class,order = is.ordered(x)))) + geom_bar()
ggplot(data=mpg, aes(x=class)) + geom_bar()
?aes
?reorder
?aes
ggplot(data=mpg, aes(x=order(class, decreasing = TRUE))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class, order(y)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class, order(class)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class, order(mpg)))) + geom_bar()
ggplot(data=mpg, aes(x=reorder(class, order(count(class))))) + geom_bar()
ggplot(data=mpg, aes(x=class::fct_infreq(mpg))) + geom_bar()
ggplot(data=mpg, aes(x=class::fct_infreq())) + geom_bar()
ggplot(data=mpg, aes(x=class::fct_infreq(class))) + geom_bar()
ggplot(data=mpg, aes(x=mpg::fct_infreq(class))) + geom_bar()
library(ggplot2)
# 막대 그래프 - 집단간의 차이를 비교
library(dplyr)
ggplot(data=mpg, aes(x=mpg::fct_infreq(class))) + geom_bar()
install.packages("forcats")
library(forcats)
ggplot(data=mpg, aes(x=forcats::fct_infreq(class))) + geom_bar()
ggplot(data=mpg, aes(x=forcats::fct_infreq(class))) + geom_col()
bar()
ggplot(data=mpg, aes(x=forcats::fct_infreq(class))) + geom_bar()
ggplot(data=mpg, aes(x=forcats::fct_infreq(class))) + geom_col()
ggplot(data=mpg, aes(x=forcats::fct_infreq(class))) + geom_bar()
ggplot(data=suv_mpg, aes(x = reorder(manufacturer, -cty_mean), y=cty_mean)) + geom_col()
ggplot(data=mpg, aes(x=reorder(class))) + geom_col()
ggplot(data=mpg, aes(x=reorder(class, count(class)))) + geom_col()
install.packages("rJava")
install.packages("memoise")
install.packages("KoNLP")
library(KoNLP)
install.packages("KoNLP")
koinstall = c('rJava', 'stringr','hash','tau','Sejong', 'RSOLite', ' devtools', 'vctrs')
install.packages(koinstall)
install.packages(koinstall)
install.packages(koinstall)
install.packages(koinstall)
install.packages(koinstall)
install.packages("vctrs")
install.packages("vctrs")
install.packages("vctrs")
install.packages("vctrs")
install.packages("vctrs")
koinstall = c('rJava', 'stringr','hash','tau','Sejong', 'RSOLite', ' devtools', 'vctrs')
install.packages(koinstall)
remotes::install_github('haven-jeon/KoNLP', upgrade="never", INSTALL_opts = c("--no-multiarch"))
remotes::install_github('haven-jeon/KoNLP', upgrade="never", INSTALL_opts = c("--no-multiarch"))
install.packages("remotes")
remotes::install_github('haven-jeon/KoNLP', upgrade="never", INSTALL_opts = c("--no-multiarch"))
library(rJava)
library(stringr)
library(KoNLP)
useSejongDic()
library(rlang)
install.packages("rlang")
library(rJava)
library(stringr)
library(KoNLP)
library(rlang)
useSejongDic()
library(rJava)
library(stringr)
library(KoNLP)
library(KoNLP)
install.packages("rlang")
library(KoNLP)
extractNoun("롯데마트 판매합니다다")
extractNoun("롯데마트 판매하하")
extractNoun("롯데마트 판매하")
txt <- readLines("hiphop.txt")
txt <- readLines("hiphop.txt")
head(txt)
txt <- str_replace_all(txt,"\\W", " ")
txt
nouns <- extractNoun(txt)
nouns
ulist(nouns)
unlist(nouns)
table(unlist(nouns))
wordCount <- table(unlist(nouns))
wordCount
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word
library(dplyr)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word
df_word <- filter(df_word, nchar(word) >= 2)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq))
df_word <- filter(df_word, nchar(word) >= 2)
top_20
df_word <- filter(df_word, nchar(word) >= 2)
library(dplyr)
library(stringr)
df_word <- filter(df_word, nchar(word) >= 2)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
library(wordcloud)
set.seed(100)
wordCount <- table(unlist(nouns))
df_word <- as.data.frame(wordCount, stringsAsFactors = T)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
df_word
set.seed(100)
pal <- brewer.pal(9, "Set1")
word <- c("인천광역시", "서울시", "대구시")
freq <- c(700, 80, 30)
wordcloud(word, freq, colors = pal, rot.per = .1)
df_word <- as.data.frame(wordCount, stringsAsFactors = F)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
top_20
word
wordcloud(top_20, freq, colors = pal, rot.per = .1)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(rJava)
library(stringr)
library(KoNLP)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
txt <- readLines("hiphop.txt")
txt <- str_replace_all(txt,"\\W", " ")
nouns <- extractNoun(txt)
nouns
wordCount <- table(unlist(nouns))
wordCount
df_word <- as.data.frame(wordCount, stringsAsFactors = F)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
set.seed(100)
pal <- brewer.pal(9, "Set1")
wordcloud(word, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
wordcloud(top_20$word, freq, colors = pal, rot.per = .1)
wordcloud(top_20, freq, colors = pal, rot.per = .1)
useSejongDic()
text <- LeadLines(file.choose())
text <- LeadLines(file.choose())
library(readxl)
movies <- read_excel("movies.xlsx")
movies <- read_excel("movies.xlsx")
movies
head(movies)
tail(movies)
names(movies)
dim(movies)
summary(movies)
max(movies$매출액)
max(movies$매출액)
max(movies$관객수)
max(movies$관객수)
max(movies$상상영횟수)
max(movies$상상영횟수)
max(movies$상상영횟수)
max(movies$상영횟수)
which.max(movies$관객수)
which.max(movies$관객수)
movies[64, 1]
movies[64]
movies[64]
movies[64, ]
which.min(movies$스크린수)
which.min(movies$스크린수)
movies[60, ]
movies$관객수수
movies$관객수수
movies$관객수
movies$관객수
hist(movies$관객수)
library(ggplot)
library(ggplot2)
names(movies)
ggplot(data = movies, aes(x = 관객수, y=매출액)) + geom_col()
ggplot(data = movies, aes(x = 관객수, y=매출액)) + geom_histogram()
ggplot(data = movies, aes(x = 관객수)) + geom_histogram()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_dotplot()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_bar()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_area()
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_line()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_line()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수)) + geom_point()
## 관객수 상영 횟수
ggplot(data = movies, aes(x = 관객수, y=상영횟수, col=대표국적)) + geom_point()
install.packages("plotly")
library(plotly)
## 관객수 상영 횟수
gp <- ggplot(data = movies, aes(x = 관객수, y=상영횟수, col=대표국적)) + geom_point()
ggplotly(go)
ggplotly(gp)
which.max(mocies$상영횟수수)
which.max(mocies$상영횟수)
which.max(movies$상영횟수)
movies[126,]
which.max(movies$곤객수/movies$상영횟수)
which.max(movies$관관객수/movies$상영횟수)
which.max(movies$관객수/movies$상영횟수)
movies[21,]
# 상영횟수당 관객수가 가장 많은 영화 찾기
movies2 %>%
mutate(상영대비관객 = 관객수/상영횟수)
# 상영횟수당 관객수가 가장 많은 영화 찾기
movies2 <- movies %>%
mutate(상영대비관객 = 관객수/상영횟수)
movies2 %>%
arrange(상영대비관객) %>%
head(10)
movies2 %>%
arrange(상영대비관객,desc) %>%
head(10)
?arrange
movies2 %>%
arrange(desc(상영대비관객)) %>%
head(10)
sort(movies$관객수 / movies$상상영횟수, decreasing = T)
sort(movies$관객수 / movies$상영횟수, decreasing = T)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관개)) %>%
head(10)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관객객)) %>%
head(10)
movies %>%
mutate(상영대비관객 = 관객수/상영횟수) %>%
arrange(desc(상영대비관객)) %>%
head(10)
name(movies)
names(movies)
### 대응 표본 T 검정
raw_d <- read.csv(file = "data/htest02d.csv", header = T)
raw_d
head(raw_d)
dim(raw_d)
setwd("C:/study/R_lang")
groupA_d <- raw_d[,1]
grouBA_d <- raw_d[,2]
mean(groupA_d)
mean(groupB_d)
groupB_d <- raw_d[,2]
mean(groupB_d)
mean(groupA_d)
mean(groupB_d)
#### 데이터 정규성 검정
shapiro.test(groupA_d)
#### 데이터 정규성 검정
d = groupA_d - groupB_d
d
shapiro.test(d)
# -> p-value가 0.1621 > 0.05이기때문에 정규분포를 따름
qqnorm(d)
qqline(d)
#### 대응표본 t검정
t.test(d)
#### 대응표본 t검정
t.test(groupA_d, groupB_d, alternative = "less")
#### 대응표본 t검정
t.test(groupA_d, groupB_d, alternative = "less", paired = T)
rawN30 = read.csv("data/htest03.csv", header = T)
head(rawN30)
tail(rawN30)
dim(rawN30)
names(rawN30)
groupA3 <- rawN30[rawN30$group == 'A', 1:2]
groupB3 <- rawN30[rawN30$group == 'B', 1:2]
groupA3
groupB3
mean(groupA3$height)
mean(groupB3$height)
cat("mean of x1:", mean_x1, "\n")
z.test <- funtion(x1,x2){
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
n_x1 = length(x1)
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
n_x2 = length(x2)
z.test <- function(x1,x2){
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less")
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
z.test(groupA3$height, groupB3$height)
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
# 위의 데이터를 t-test 했을 경우
# 분산
var.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T, conf.level = 0.95)
#####################################################
# ANOVA 분석 : 여러 집단의 평균차이 검정
# ANOVA 분석을 위해서는 lawstat 패키지가 핑요함
install.packages("lawstat")
library(lawstat)
raw_anova <- read.csv(file= "data/htest04.csv", header = T)
head(raw_anova)
tatil(raw_anova)
tail(raw_anova)
groupA4 <- raw_anova[raw_anova$group=='A', 1:2]
groupB4 <- raw_anova[raw_anova$group=='B', 1:2]
groupC4 <- raw_anova[raw_anova$group=='C', 1:2]
mean(groupA4[,2])
mean(groupB4[,2])
mean(groupC4[,2])
# 정규성 검정
shapiro.test(groupA4[,2])
shapiro.test(groupB4[,2])
shapiro.test(groupC4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupB4[,2])
qqline(groupB4[,2])
qqnorm(groupC4[,2])
qqline(groupC4[,2])
anova(raw_anova)
anova(groupA4$height, groupB4$height, groupC4$height)
levene.test(raw_anova$height, raw_anova$group)
# p-value = 0.3298이므로 분산이 동일하다.
bartlett.test(height~group, data=raw_anova)
# p-value = 0.3435이므로 세집단간의 분산이 동일하다.
rawAnova <- aov(height ~ group, data= raw_anova)
summary(rawAnova)
head(raw_chisq)
###########################################
# 분할표를 이용한 연관성 분석
raw_chisq <- read.csv(file="data/htest05.csv", header = T)
head(raw_chisq)
tail(raw_chisq)
raw_Table <- table(raw_chisq)
raw_Table
# 카이제곱 테스트
chisq.test(raw_Table)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = F)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = FALSE)
# 카이제곱 테스트
chisq.test(raw_Table, correct = FALSE)
