n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
n_x1 = length(x1)
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
n_x2 = length(x2)
z.test <- function(x1,x2){
n_x1 = length(x1)
n_x2 = length(x2)
mean_x1 = mean(x1)
mean_x2 = mean(x2)
cat("\n")
cat("\tTwo Sample z-test\n")
cat("\n")
cat("mean of x1:", mean_x1, "\n")
cat("mean of x2:", mean_x2, "\n")
var_x1 = var(x1)
var_x2 = var(x2)
z = (mean_x1 - mean_x2)/sqrt((var_x1/n_x1)+(var_x2/n_x2))
abs_z = abs(z)
cat("z =", abs_z, "\n")
p_value = 1-pnorm(abs_z)
cat("p-value =", p_value)
}
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less")
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
z.test(groupA3$height, groupB3$height)
z.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = F)
# 위의 데이터를 t-test 했을 경우
# 분산
var.test(groupA3$height, groupB3$height)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T)
t.test(groupA3$height, groupB3$height, alternative = "less", var.equal = T, conf.level = 0.95)
#####################################################
# ANOVA 분석 : 여러 집단의 평균차이 검정
# ANOVA 분석을 위해서는 lawstat 패키지가 핑요함
install.packages("lawstat")
library(lawstat)
raw_anova <- read.csv(file= "data/htest04.csv", header = T)
head(raw_anova)
tatil(raw_anova)
tail(raw_anova)
groupA4 <- raw_anova[raw_anova$group=='A', 1:2]
groupB4 <- raw_anova[raw_anova$group=='B', 1:2]
groupC4 <- raw_anova[raw_anova$group=='C', 1:2]
mean(groupA4[,2])
mean(groupB4[,2])
mean(groupC4[,2])
# 정규성 검정
shapiro.test(groupA4[,2])
shapiro.test(groupB4[,2])
shapiro.test(groupC4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupA4[,2])
qqline(groupA4[,2])
qqnorm(groupA4[,2])
qqline(groupB4[,2])
qqnorm(groupB4[,2])
qqline(groupB4[,2])
qqnorm(groupC4[,2])
qqline(groupC4[,2])
anova(raw_anova)
anova(groupA4$height, groupB4$height, groupC4$height)
levene.test(raw_anova$height, raw_anova$group)
# p-value = 0.3298이므로 분산이 동일하다.
bartlett.test(height~group, data=raw_anova)
# p-value = 0.3435이므로 세집단간의 분산이 동일하다.
rawAnova <- aov(height ~ group, data= raw_anova)
summary(rawAnova)
head(raw_chisq)
###########################################
# 분할표를 이용한 연관성 분석
raw_chisq <- read.csv(file="data/htest05.csv", header = T)
head(raw_chisq)
tail(raw_chisq)
raw_Table <- table(raw_chisq)
raw_Table
# 카이제곱 테스트
chisq.test(raw_Table)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = F)
# 카이제곱 테스트
chisq.test(raw_Table, crrect = FALSE)
# 카이제곱 테스트
chisq.test(raw_Table, correct = FALSE)
library(caret)
rawdata <- read.csv(file="heart.csv", header = T)
str(rawdata)
# 타겟(정답, class) 변수의 범주화
rawdata$target <- as.factor(rawdata$target)
str(rawdata)
# 범주형 독립변수는 범주화
newdata <- rawdata
factVar <- c("sex","cp","fps","restecg","exang","ca","thal")
factVar <- c("sex","cp","fbs","restecg","exang","ca","thal")
newdata_[,factVar] = lapply(newdata[, factVar], factor)
newdata[,factVar] = lapply(newdata[, factVar], factor)
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata$trestbps <- scale(rawdata$age)
rawdata$chol <- scale(rawdata$age)
rawdata$thalach <- scale(rawdata$age)
rawdata$slope <- scale(rawdata$age)
# 타겟(정답, class) 변수의 범주화
rawdata$target <- as.factor(rawdata$target)
# 연속형 독립변수에 대해서는 표준화를 한다.
rawdata$age <- scale(rawdata$age)
rawdata$trestbps <- scale(rawdata$age)
rawdata$chol <- scale(rawdata$age)
rawdata$thalach <- scale(rawdata$age)
rawdata$slope <- scale(rawdata$age)
# 범주형 독립변수는 범주화
newdata <- rawdata
factVar <- c("sex","cp","fbs","restecg","exang","ca","thal")
newdata[,factVar] = lapply(newdata[, factVar], factor)
# 트레이닝 - 테스트 데이터 분할
set.seed(2020)
datatotal <- sort(sample(nrow(newdata)))
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*0.7))
train <- newdata[datatotal, ]
train <- newdata[-datatotal, ]
train <- newdata[datatotal, ]
test <- newdata[-datatotal, ]
# 학습모델 생성
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LogitBoost",
trControl = ctrl,
metric = "Accuracy")
logitFit
# 트레이닝 - 테스트 데이터 분할
set.seed(2020)
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*0.7))
train <- newdata[datatotal, ]
test <- newdata[-datatotal, ]
# 학습모델 생성
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LogitBoost",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
# "LogitBoost", "LMT", "plr", "regLogistic"
ctrl <- trainControl(method= "repeatedcv", repeats = 5)
logitFit <- train(target ~ .,
data = train,
method = "LMT",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "regLogistic",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "plr",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
# 변수의 중요도
importance_logit <- varImp(logitFit, scale=F)
plot(importance_logit)
logitFit <- train(target ~ .,
data = train,
method = "regLogistic",
trControl = ctrl,
metric = "Accuracy")
# nIter : 반복 횟수
logitFit
plot(logitFit)
# 모델을 이용한 예측
pred_test <- predict(logitFit, newdata = test)
confusionMatrix(pred_test, test$target)
library(caret)
rawdata <- read.csv(file="wine.csv", header=T)
rawdata$Class <- as.factor(rawdata$Class)
str(rawdata)
analdata <- rawdata
set.seed(2020)
datatotal <- sort(sample(nrow(analdata), nrow(analdata)*.7))
train <- rawdata[datatotal,]
test <- rawdata[-datatotal,]
str(train)
# Decision Tree 학습
install.packages("tree")
library(tree)
treeRaw <- tree(Class~.,
data=train)
plot(treeRaw)
text(treeRaw)
# 가장 적합한 사이즈 찾기(Tree Size)
# prune.misclass : 오분류율이 많은 값을 기준으로 가지치기를 함
cv_tree <- cv.tree(treeRaw, FUN = prune.misclass)
plot(cv_tree)
# 가지치기(가지가 너무 많으면 성능상의 문제)
# best값은 cv에서 측정된 값을 적용
prune_tree <- prune.misclass(treeRaw,best = 4)
plot(prune_tree)
text(prune_tree, pretty = 0)
pred <- predict(prune_tree, test, type='class')
confusionMatrix(pred, test$Class)
plot(treeRaw)
text(treeRaw)
# Random Forest 학습
ctrl <- trainControl(mehod="repeatedcv", repeats = 5)
# Random Forest 학습
ctrl <- trainControl(method="repeatedcv", repeats = 5)
rfFit <- train(Class ~.,
data=train,
method="rf",
trControl = ctrl,
preprocess - c("center", "scale"),
metric="Accuracy"
)
rfFit <- train(Class ~.,
data=train,
method="rf",
trControl = ctrl,
preProcess - c("center", "scale"),
metric="Accuracy"
)
# Random Forest 학습
ctrl <- trainControl(method="repeatedcv", repeats = 5)
rfFit <- train(Class ~.,
data=train,
method="rf",
trControl = ctrl,
preProcess - c("center", "scale"),
metric="Accuracy"
)
rfFit <- train(Class ~.,
data=train,
method="rf",
trControl = ctrl,
preProcess = c("center", "scale"),
metric="Accuracy")
rfFit
plot(rfFit)
pred_test <- predic(rfFit, newdata = test)
pred_test <- predict(rfFit, newdata = test)
confusionMatrix(pred_test,test$Class)
# 변수의 중요도
importance_rf <- varImp(rfFit, scale=F)
plot(importance_rf)
library(caret)
rawdata <- read.csv(file="wine.csv", header = T)
rawdata$Class <- as.factor(rawdata$Class)
str(rawdata)
# 테스트 데이터 분할
analdata <- rawdata
set.seed(2020)
datatotal <- sort(sample(nrow(analdata), nrow(analdata)*.7))
train <- rawdata[datatotal,]
test <- rawdata[-datatotal,]
str(train)
# 섢형 서포트 벡터 머신
ctrl <- trainControl(method = "repeatedcv", repeats = 5)
svm_linear_fit <- train(Class ~ .,
data=train,
method="svmLinear",
trControl = ctrl,
preProcess = c("center", "scale"),
metric="Accuracy")
svm_linear_fit
plot(svm_linear_fit)
# prediction
pred_test <- predict(svm_linear_fit, newdata = test)
confusionMatrix(pred_test, test$Class)
#
importance_linear <- varImp(svm_linear_fit, scale = F)
plot(importance_linear)
# 비선형 서포트 벡터 머신
ctrl <- trainControl(method = "repeatedcv", repeats = 5)
svm_poly_fit = train(Class ~ .,
data = train,
mehod = "svmPoly",
trControl = ctrl,
preProcess = c("center", "scale"),
metric="Accuracy")
svm_poly_fit
plot(svm_linear_fit)
plot(svm_linear_fit)
plot(svm_poly_fit)
# 비선형 서포트 벡터 머신
ctrl <- trainControl(method = "repeatedcv", repeats = 5)
svm_poly_fit = train(Class ~ .,
data = train,
mehod = "svmPoly",
trControl = ctrl,
preProcess = c("center", "scale"),
metric="Accuracy")
svm_poly_fit
plot(svm_poly_fit)
confusionMatrix(pred_test, test$Class)
# 변수의 중요도
importance_poly <- varImp(svm_poly_fit, scale=F)
plot(importance_poly)
rawdata <- read.csv("univercity.csv", header=T)
rawdata <- read.csv("university.csv", header=T)
rawdata1 <- read.csv("university.csv", header=T)
str(rawdata1)
is.na(rawdata1$GRE.Score)
sum(is.na(rawdata1$GRE.Score))
sum(is.na(rawdata1$TOEFL.Score))
sum(is.na(rawdata1$University.Rating))
sum(is.na(rawdata1$SOP))
sum(is.na(rawdata1$LOR))
sum(is.na(rawdata1$CGPA))
sum(is.na(rawdata1$Research))
sum(is.na(rawdata1$Chance.of.Admit))
unique(rawdata1)
unique(rawdata1$GRE.Score)
unique(rawdata1$TOFLE)
unique(rawdata1$TOEFL.Score)
unique(rawdata1$University.Rating)
unique(rawdata1$SOP)
unique(rawdata1$LOR)
unique(rawdata1$Research)
unique(rawdata1$Chance.of.Admit)
research_table <- table(rawdata1$Research)
research_table
max(rawdata1$Chance.of.Admit)
min(rawdata1$Chance.of.Admit)
hist(rawdata1$GRE.Score, main="GRE score histogram", clab="GRE scoer", col="orange")
hist(rawdata1$GRE.Score, main="GRE score histogram", clab="GRE scoer", col="orange")
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$TOEFL.Score, main="TOEFL", xlab="toefl scoer", col="green")
# 변수 별 히스토그램
par(mfrow=c(3,2), mar=c(3,4,4,2))
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$TOEFL.Score, main="TOEFL", xlab="toefl scoer", col="green")
hist(rawdata1$SOP, main="SOP histogram", xlab="sop", col="blue")
hist(rawdata1$CGPA, main="CGPA score histogram", xlab="cgpa", col="red")
hist(rawdata1$LOR, main="LOR histogram", xlab="LOR", col="puple")
hist(rawdata1$Chance.of.Admit, main="C.o.A histogram", xlab="chance_of_admit", col="brown")
hist(rawdata1$LOR, main="LOR histogram", xlab="LOR", col="yellow")
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$TOEFL.Score, main="TOEFL", xlab="toefl scoer", col="green")
hist(rawdata1$SOP, main="SOP histogram", xlab="sop", col="blue")
hist(rawdata1$CGPA, main="CGPA score histogram", xlab="cgpa", col="red")
hist(rawdata1$LOR, main="LOR histogram", xlab="LOR", col="yellow")
# 변수 별 히스토그램
par(mfrow=c(3,2), mar=c(3,4,4,2))
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$TOEFL.Score, main="TOEFL", xlab="toefl scoer", col="green")
hist(rawdata1$SOP, main="SOP histogram", xlab="sop", col="blue")
hist(rawdata1$CGPA, main="CGPA score histogram", xlab="cgpa", col="red")
hist(rawdata1$LOR, main="LOR histogram", xlab="LOR", col="yellow")
hist(rawdata1$Chance.of.Admit, main="C.o.A histogram", xlab="chance_of_admit", col="brown")
# 산점
plot(rawdata1)
# 트레이닝 / 테스트 데이터 나누기
set.seed(2020)
newdata <- rawdata1
train_ratio <- 0.7
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*train_ratio))
# 트레이닝 / 테스트 데이터 나누기
set.seed(2020)
newdata <- rawdata1
train_ratio <- 0.7
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*train_ratio))
train <- newdata[datatotal,]
test <- newdata[-datatotal,]
# 회귀분석으로 대학원 합격률 예측(caret패키지 사용)
library(caret)
# 로지스틱 회귀분석
ctrl <- trainControl(method = "repeatedcv", repeat=5)
# 로지스틱 회귀분석
ctrl <- trainControl(method = "repeatedcv", repeat=5)
# 로지스틱 회귀분석
ctrl <- trainControl(method = "repeatedcv", repeat=5)
# 로지스틱 회귀분석
ctrl <- trainControl(method = "repeatedcv", repeats=5)
# 로지스틱 회귀분석
# 오차최소값, rmse
ctrl <- trainControl(method = "repeatedcv", repeats=5)
logistic_fit <- train(Chance.of.Admit ~ .,
data=train,
method="glm",
trControl = ctrl,
preProcess = c("center", "scale"),
metric = "RMSE")
logistic_fit
plot(logistic_fit)
# Rsquared(R제곱값) : 전체 변동성 중 설명가능한 변동성 비율율
logistic_pred <- predict(logistic_fit, newdata=test)
logistic_pred
# FALSE = 0, TRUE = 1, 결과가 0이면 결측치 없음
sum(is.na(rawdata1$GRE.Score))
sum(is.na(rawdata1$TOEFL.Score))
sum(is.na(rawdata1$University.Rating))
sum(is.na(rawdata1$SOP))
sum(is.na(rawdata1$LOR))
sum(is.na(rawdata1$CGPA))
sum(is.na(rawdata1$Research))
sum(is.na(rawdata1$Chance.of.Admit))
unique(rawdata1$GRE.Score)
unique(rawdata1$TOEFL.Score)
unique(rawdata1$University.Rating)
unique(rawdata1$SOP)
unique(rawdata1$LOR)
unique(rawdata1$Research)
unique(rawdata1$Chance.of.Admit)
research_table <- table(rawdata1$Research)
research_table
# 타겟변수 최대,최소값 확인
max(rawdata1$Chance.of.Admit)
min(rawdata1$Chance.of.Admit)
# 변수 별 히스토그램
par(mfrow=c(3,2), mar=c(3,4,4,2))
hist(rawdata1$GRE.Score, main="GRE score histogram", xlab="GRE scoer", col="orange")
hist(rawdata1$TOEFL.Score, main="TOEFL", xlab="toefl scoer", col="green")
hist(rawdata1$SOP, main="SOP histogram", xlab="sop", col="blue")
hist(rawdata1$CGPA, main="CGPA score histogram", xlab="cgpa", col="red")
hist(rawdata1$LOR, main="LOR histogram", xlab="LOR", col="yellow")
hist(rawdata1$Chance.of.Admit, main="C.o.A histogram", xlab="chance_of_admit", col="brown")
# 산점도 Acceptance rate
plot(rawdata1)
